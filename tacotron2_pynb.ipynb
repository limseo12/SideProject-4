{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMF8b/twDGxi1MKzX95sC5c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/limseo12/SideProject-4/blob/main/tacotron2_pynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-m2zf3Lf_KKr"
      },
      "outputs": [],
      "source": [
        "!pip install torch\n",
        "!pip install torchaudio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "import re\n",
        "import time\n",
        "from torch.autograd import Variable\n",
        "from math import sqrt\n",
        "\n",
        "print('Import 완료')"
      ],
      "metadata": {
        "id": "7ZgmFmj4_lHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "0. Hyperparameter 설정 및 device 확인\\\n",
        "batch size는 큰 값을 갖는게 좋음"
      ],
      "metadata": {
        "id": "kwFKFyfT_p_b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HParams():\n",
        "    def __init__(self):\n",
        "        self.n_mel_channels = 80    #Mel-spectrogram의 착수 80차\n",
        "\n",
        "        ################################\n",
        "        # Model Parameters             #\n",
        "        ################################\n",
        "        self.symbols_embedding_dim=512    #텍스트에서 인베딩, 룩업테이블을 통해서 임베딩한다\n",
        "\n",
        "        # Encoder parameters\n",
        "        self.encoder_kernel_size=5\n",
        "        self.encoder_n_convolutions=3   #커널 사이즈가 5인 컨볼루션을 3개 쌓는다\n",
        "        self.encoder_embedding_dim=512    #LSTM을 512로 쌓는게 뒤쪽에 나온다\n",
        "\n",
        "        # Decoder parameters\n",
        "        self.n_frames_per_step=1  # currently only 1 is supported   ,타코트론의 경우 멜을 출력으로 낼 때 하나 이상의 멜 스펙트롤을 뱉는다,타코트론2에서는 계산속도는 빠르나 성능이 떨어질 수 있기에 1로고정\n",
        "        self.decoder_rnn_dim=1024   \n",
        "        self.prenet_dim=256\n",
        "        self.max_decoder_steps=1000\n",
        "        self.gate_threshold=0.5\n",
        "        self.p_attention_dropout=0.1\n",
        "        self.p_decoder_dropout=0.1\n",
        "\n",
        "        # Attention parameters\n",
        "        self.attention_rnn_dim=1024\n",
        "        self.attention_dim=128\n",
        "\n",
        "        # Location Layer parameters\n",
        "        self.attention_location_n_filters=32\n",
        "        self.attention_location_kernel_size=31\n",
        "\n",
        "        # Mel-post processing network parameters\n",
        "        self.postnet_embedding_dim=512\n",
        "        self.postnet_kernel_size=5\n",
        "        self.postnet_n_convolutions=5\n",
        "\n",
        "        ################################\n",
        "        # Optimization Hyperparameters #\n",
        "        ################################\n",
        "        self.use_saved_learning_rate=False\n",
        "        self.learning_rate=1e-3\n",
        "        self.weight_decay=1e-6\n",
        "        self.grad_clip_thresh=1.0\n",
        "        self.batch_size=4\n",
        "        self.mask_padding=True  # set model's padded outputs to padded values\n",
        "    \n",
        "    \n",
        "hparams = HParams()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "  print('GPU를 사용할 준비가 되었습니다.')\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "  print('CPU 모드입니다. GPU 설정으로 변경해주세요.')"
      ],
      "metadata": {
        "id": "ySsikYwf_m9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Dataset (LJ Speech) 다운로드\\\n",
        "Public data : LJ-Speech, VCTK, LibriTTS, (blizzard challenge data)"
      ],
      "metadata": {
        "id": "qDd07Tun_vk_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LJSpeech_url = 'https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2'   #오디오북을 녹음한 DB, 어떠한 모델을 학습해도 소리가 잘 나온다.VCTK는 데이터가 좀 더 작아 음질이 좋진않다,LibriTTS는 db사이즈가 너무 커서 일부만 사용하거나 작은 셋을 사용하는 걸 추천한다.\n",
        "train_dataset = torchaudio.datasets.LJSPEECH(\"\", url=LJSpeech_url, download=True)\n",
        "\n",
        "print('Download 완료')"
      ],
      "metadata": {
        "id": "FRKFwMaw_yKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Dataset 전처리"
      ],
      "metadata": {
        "id": "8MFR7RnN_0v2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = \" abcdefghijklmnopqrstuvwxyz'.?\"  # P: Padding, E: EOS. 각각의 voca가 숫자로 변환되게 된다.\n",
        "char2idx = {char: idx for idx, char in enumerate(vocab)}\n",
        "idx2char = {idx: char for idx, char in enumerate(vocab)}\n",
        "\n",
        "\n",
        "def text_normalize(text):   #영어 같은 경우에는 대문자 소문자가 있기에 소문자로 맞춰준다. 실제로는 이 코드 보다 좀 더 복잡하다.\n",
        "    text = ''.join(char for char in unicodedata.normalize('NFD', text)\n",
        "                   if unicodedata.category(char) != 'Mn')  # Strip accents\n",
        "\n",
        "    text = text.lower()\n",
        "    text = re.sub(\"[^{}]\".format(vocab), \" \", text)\n",
        "    text = re.sub(\"[ ]+\", \" \", text)\n",
        "    return text\n",
        "\n",
        "\n",
        "class Collate():    #기존의 NVDIA 코드에서 조금 더 변형된 코드,\n",
        "  def __init__(self):\n",
        "    # n_mels=80은 몇 차에 멜 스펙트럼을 사용할 것인가,\n",
        "    #win_length는 Short-Time Analysis를 하기위한 윈도우길이,타임축에서 씌우는 것\n",
        "    #hop_length=256는 win_length를 256만큼 조금씩 이동시킨다,\n",
        "    #f_min,f_max은 어느 정도의 바운더리를 설정한 것 , 값을 키우면 성능이 좋아지지만 계산할 때 안좋은 점이 있다.\n",
        "    #n_fft는 fft사이즈이다,주파수 도메인에서 몇 차를 할 것인가\n",
        "    self.wav_to_mel = torchaudio.transforms.MelSpectrogram(sample_rate=22050, n_mels=80, win_length=1024, hop_length=256, f_min=0.0, f_max=8000.0, n_fft=1024)\n",
        "\n",
        "  def __call__(self, batch):\n",
        "    # batch: N_batch * [wav, sample_rate, text, text_normalized]\n",
        "    #소리가 안나오는 경우가 많아서 잘 확인 해보자.\n",
        "    #긴 것 부터 차례대로 내려가는 경우에 계산이 효율적으로 되기에 솔팅을 한는 것을 볼 수 있다. \n",
        "    mel_list = []\n",
        "    for data in batch:\n",
        "      wav = data[0]\n",
        "      mel_list.append(self.wav_to_mel(wav).squeeze())\n",
        "    input_lengths, ids_sorted_decreasing = torch.sort(torch.LongTensor([len(data[3]) for data in batch]), dim=0, descending=True)\n",
        "    mel_lengths, ids_sorted_mel = torch.sort(torch.LongTensor([mel.shape[1] for mel in mel_list]), dim=0, descending=True)\n",
        "\n",
        "    max_input_len = input_lengths[0]\n",
        "    max_target_len = mel_lengths[0]\n",
        "    #제일 긴 단계에 따라서 패딩을 해주는 단계\n",
        "    text_padded = torch.LongTensor(len(batch), max_input_len)\n",
        "    mel_padded = torch.FloatTensor(len(batch), 80, max_target_len)\n",
        "    #스탑토큰과 관련된 부분, 스탑토큰 , 멜과 관련된 것 , 텍스트 관련된 것도 3가지 다 준비한다.\n",
        "    gate_padded = torch.FloatTensor(len(batch), max_target_len)\n",
        "    output_lengths = torch.LongTensor(len(batch))\n",
        "\n",
        "    text_padded.zero_()\n",
        "    mel_padded.zero_()\n",
        "    gate_padded.zero_()\n",
        "\n",
        "    for i in range(len(ids_sorted_decreasing)):\n",
        "        _, _, _, text = batch[ids_sorted_decreasing[i]]\n",
        "        mel = mel_list[ids_sorted_decreasing[i]]\n",
        "        mel = self.dynamic_range_compression(mel)\n",
        "        mel_padded[i, :, :mel.size(1)] = mel\n",
        "        #끝나는 지점을 1이 되게해서 확인 할 수 있도록 한다.\n",
        "        gate_padded[i, mel.size(1)-1:] = 1\n",
        "        #아웃풋 사이즈를 통하여 게이트 값들을 구하게 된다.\n",
        "        output_lengths[i] = mel.size(1)\n",
        "        text = text_normalize(text)\n",
        "        text = [char2idx[char] for char in text]\n",
        "        text_norm = torch.IntTensor(text)\n",
        "        text_padded[i, :len(text)] = text_norm\n",
        "\n",
        "    return text_padded, input_lengths, mel_padded, gate_padded, output_lengths\n",
        "\n",
        "    #고주파 내용들이 너무 열화가 되서 고려가 안 될 수 있어서 그걸 살려주기 위해서 한 것.\n",
        "  def dynamic_range_compression(self, x, C=1, clip_val=1e-5):\n",
        "    return torch.log(torch.clamp(x, min=clip_val) * C)\n",
        "\n",
        "collate_fn = Collate()\n",
        "#파이 토키의 데이터로더를 사용하여 lj스피치를 받고 데이터를 처리하게 된다.\n",
        "#barch_size같은 경우에는 LJ경우에 8만 되도 잘 되는 경우가 많은데 기본적으로 128정도면 성능이 잘 나온다 보면 된다.\n",
        "#suffle은 당연히 해주고\n",
        "#drop_last는 남은 것을 버릴 건지 아닐 건지 물어보는 건데 버리지 않고 다 사용하겠다는 False\n",
        "train_loader = DataLoader(train_dataset, batch_size=hparams.batch_size, shuffle=True, drop_last=False, collate_fn=collate_fn)\n",
        "\n",
        "print('Data 전처리 완료')"
      ],
      "metadata": {
        "id": "apRj6FjI_2f1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Tacotron2 모델 구현\\\n",
        "3-0) Basic model"
      ],
      "metadata": {
        "id": "VhQqF_6y_5me"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_mask_from_lengths(lengths):\n",
        "    max_len = torch.max(lengths).item()\n",
        "    ids = torch.arange(0, max_len, out=torch.cuda.LongTensor(max_len))\n",
        "    mask = (ids < lengths.unsqueeze(1)).bool()\n",
        "    return mask\n",
        "\n",
        "#텐서 x를 gpu에 올리는 과정.\n",
        "def to_gpu(x):\n",
        "    x = x.contiguous() #연산과정에서 메모리에 올려진 순서에 따라 발생할 수 있는 비효율성 혹은 최악의 경우 에러를 방지하기 위함\n",
        "                        #예를들어 transpose의 경우 메모리에 올려진 순서가 연속성이 깨지게되는데 메모리 접근 성능이 비효율적으로됨\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        x = x.cuda(non_blocking=True)\n",
        "    return torch.autograd.Variable(x)\n",
        "\n",
        "#리니어한 레이어 1나\n",
        "class LinearNorm(torch.nn.Module):\n",
        "  def __init__(self, in_dim, out_dim, bias=True, w_init_gain='linear'):\n",
        "      super(LinearNorm, self).__init__()\n",
        "      self.linear_layer = torch.nn.Linear(in_dim, out_dim, bias=bias)\n",
        "\n",
        "      torch.nn.init.xavier_uniform_(\n",
        "          self.linear_layer.weight,\n",
        "          gain=torch.nn.init.calculate_gain(w_init_gain))\n",
        "\n",
        "  def forward(self, x):\n",
        "      return self.linear_layer(x)\n",
        "\n",
        "#1d 컨볼루션을 한다고 생각하면 된다.\n",
        "class ConvNorm(torch.nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, kernel_size=1, stride=1,\n",
        "                padding=None, dilation=1, bias=True, w_init_gain='linear'):\n",
        "      super(ConvNorm, self).__init__()\n",
        "      if padding is None:\n",
        "          assert(kernel_size % 2 == 1)\n",
        "          padding = int(dilation * (kernel_size - 1) / 2)\n",
        "\n",
        "      self.conv = torch.nn.Conv1d(in_channels, out_channels,\n",
        "                                  kernel_size=kernel_size, stride=stride,\n",
        "                                  padding=padding, dilation=dilation,\n",
        "                                  bias=bias)\n",
        "\n",
        "      torch.nn.init.xavier_uniform_(\n",
        "          self.conv.weight, gain=torch.nn.init.calculate_gain(w_init_gain))\n",
        "\n",
        "  def forward(self, signal):\n",
        "      conv_signal = self.conv(signal)\n",
        "      return conv_signal"
      ],
      "metadata": {
        "id": "0_fgaaoU_7o8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3-1) Encoder"
      ],
      "metadata": {
        "id": "xwbpXmZl__kw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  \"\"\"Encoder module:\n",
        "      - Three 1-d convolution banks\n",
        "      - Bidirectional LSTM\n",
        "  \"\"\"\n",
        "  def __init__(self, hparams):\n",
        "      super(Encoder, self).__init__()\n",
        "\n",
        "      convolutions = []\n",
        "      for _ in range(hparams.encoder_n_convolutions):\n",
        "      ##시퀀셜하게 ConvNorm 을 쌓는다.    \n",
        "          conv_layer = nn.Sequential(\n",
        "              ConvNorm(hparams.encoder_embedding_dim,\n",
        "                        hparams.encoder_embedding_dim,\n",
        "                        kernel_size=hparams.encoder_kernel_size, stride=1,\n",
        "                        padding=int((hparams.encoder_kernel_size - 1) / 2),\n",
        "                        dilation=1, w_init_gain='relu'),\n",
        "      #아까 보았던 대로 ConvNorm을 하고 배치노멀라이제이션을 한다.\n",
        "             \n",
        "              nn.BatchNorm1d(hparams.encoder_embedding_dim))\n",
        "          convolutions.append(conv_layer)\n",
        "      self.convolutions = nn.ModuleList(convolutions)\n",
        "      #LSTM 그대로 사용, 인코더에서는 양방향 모두 사용하는 것이 성능향상에 도움이 되어 bidirectional을 사용한다.\n",
        "      self.lstm = nn.LSTM(hparams.encoder_embedding_dim,\n",
        "                          int(hparams.encoder_embedding_dim / 2), 1,\n",
        "                          batch_first=True, bidirectional=True)\n",
        "\n",
        "  def forward(self, x, input_lengths):\n",
        "      for conv in self.convolutions:\n",
        "      #액티베이션을 걸고 (F.rely) 드랍아웃 하는 형태  \n",
        "          x = F.dropout(F.relu(conv(x)), 0.5, self.training)\n",
        "\n",
        "      x = x.transpose(1, 2)\n",
        "\n",
        "      # pytorch tensor are not reversible, hence the conversion\n",
        "      input_lengths = input_lengths.cpu().numpy()\n",
        "      #팩 패디드 시퀀스를 하게 되는 과정.병렬 처리로 계산을 좀 더 빨리 하기 위해서 시퀀스를 패킹하는 과정\n",
        "      #길이에 따라 소팅하는 과정이라고 보면 될 것 같다\n",
        "      x = nn.utils.rnn.pack_padded_sequence(\n",
        "          x, input_lengths, batch_first=True) #병렬 처리를 위해 padded된 sequence들을 packing하는 과정, 일종의 길이에 따른 sorting 과정 더 빠른 연산 가능\n",
        "\n",
        "      self.lstm.flatten_parameters()\n",
        "      outputs, _ = self.lstm(x)\n",
        "      #패드패킨스 시퀀스를 한번 더 진행한다.논문과 똑같이 인코더를 구하게 된다.\n",
        "      outputs, _ = nn.utils.rnn.pad_packed_sequence(\n",
        "          outputs, batch_first=True)\n",
        "\n",
        "      return outputs\n",
        "#forward와 inference로 나뉜 이유는 forward는 학습할 때 사용하고 inference는 실제 학습된 모델을 인퍼런스 할 때 나오는 것\n",
        "#inference 같은 경우에는 배치사이즈가 1이라고 볼 수 있기 때문에 굳이 택 패지드 시퀀스가 들어가 있지 않은 형태로 구현이 되어 있는 것을 볼 수 있다.\n",
        "  def inference(self, x):\n",
        "      for conv in self.convolutions:\n",
        "          x = F.dropout(F.relu(conv(x)), 0.5, self.training)\n",
        "\n",
        "      x = x.transpose(1, 2)\n",
        "\n",
        "      self.lstm.flatten_parameters()\n",
        "      outputs, _ = self.lstm(x)\n",
        "\n",
        "      return outputs"
      ],
      "metadata": {
        "id": "kIozRVX9ABtl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3-2) Attention"
      ],
      "metadata": {
        "id": "9YSYmYk4AEMY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LocationLayer(nn.Module):\n",
        "  def __init__(self, attention_n_filters, attention_kernel_size,\n",
        "                attention_dim):\n",
        "      super(LocationLayer, self).__init__()\n",
        "      padding = int((attention_kernel_size - 1) / 2)\n",
        "      self.location_conv = ConvNorm(2, attention_n_filters,\n",
        "                                    kernel_size=attention_kernel_size,\n",
        "                                    padding=padding, bias=False, stride=1,\n",
        "                                    dilation=1)\n",
        "      self.location_dense = LinearNorm(attention_n_filters, attention_dim,\n",
        "                                        bias=False, w_init_gain='tanh')\n",
        "\n",
        "  def forward(self, attention_weights_cat):\n",
        "      processed_attention = self.location_conv(attention_weights_cat)\n",
        "      processed_attention = processed_attention.transpose(1, 2)\n",
        "      processed_attention = self.location_dense(processed_attention)\n",
        "      return processed_attention\n",
        "\n",
        "#쿼리 메모리는 디코더 아웃풋, 수식에서 h값 이다 메모리 값은 인코더 인베딩에 시퀀스 수식에 있는 xn이다.\n",
        "#어텐션 웨이트는 합성에서는 얼라이먼트라고 표현하는데 밑에 수식에서보이는 y값이라 보면 된다.\n",
        "#c는 컨텍스트 벡터, 어텐션은 최종 출력이라고 수있다\n",
        "class Attention(nn.Module):\n",
        "  def __init__(self, attention_rnn_dim, embedding_dim, attention_dim,\n",
        "                attention_location_n_filters, attention_location_kernel_size):\n",
        "      super(Attention, self).__init__()\n",
        "      \n",
        "      self.query_layer = LinearNorm(attention_rnn_dim, attention_dim,\n",
        "                                    bias=False, w_init_gain='tanh')\n",
        "      self.memory_layer = LinearNorm(embedding_dim, attention_dim, bias=False,\n",
        "                                      w_init_gain='tanh')\n",
        "      self.v = LinearNorm(attention_dim, 1, bias=False)\n",
        "      self.location_layer = LocationLayer(attention_location_n_filters,\n",
        "                                          attention_location_kernel_size,\n",
        "                                          attention_dim)\n",
        "      self.score_mask_value = -float(\"inf\")\n",
        "\n",
        "\n",
        "  def get_alignment_energies(self, query, processed_memory,\n",
        "                              attention_weights_cat):\n",
        "      \"\"\"\n",
        "      PARAMS\n",
        "      ------\n",
        "      query: decoder output (batch, n_mel_channels * n_frames_per_step)\n",
        "      processed_memory: processed encoder outputs (B, T_in, attention_dim)\n",
        "      attention_weights_cat: cumulative and prev. att weights (B, 2, max_time)\n",
        "\n",
        "      RETURNS\n",
        "      -------\n",
        "      alignment (batch, max_time)\n",
        "      \"\"\"\n",
        "#에너지 펑션 ,e를 구하는 과정, 디코더 아웃풋에 웨이트를 하나 주고 로케이션 레이어는 앞의 수식에서의 f 와 관련된 내용.\n",
        "#쿼리+ 어텐션+ 메모리 로 이루어지게 된다.\n",
        "      processed_query = self.query_layer(query.unsqueeze(1))\n",
        "      processed_attention_weights = self.location_layer(attention_weights_cat)\n",
        "      energies = self.v(torch.tanh(\n",
        "          processed_query + processed_attention_weights + processed_memory))\n",
        "\n",
        "      energies = energies.squeeze(-1)\n",
        "      return energies\n",
        "#forward 부분에서 용어가 살짝다른데 에너지가 얼라이먼트라고 되어있고 어텐션웨이트가 얼라이먼트 라고 나와있는 것을 볼 수 있다.\n",
        "  def forward(self, attention_hidden_state, memory, processed_memory,\n",
        "              attention_weights_cat, mask):\n",
        "      \"\"\"\n",
        "      PARAMS\n",
        "      ------\n",
        "      attention_hidden_state: attention rnn last output\n",
        "      memory: encoder outputs\n",
        "      processed_memory: processed encoder outputs\n",
        "      attention_weights_cat: previous and cummulative attention weights\n",
        "      mask: binary mask for padded data\n",
        "      \"\"\"\n",
        "      alignment = self.get_alignment_energies(\n",
        "          attention_hidden_state, processed_memory, attention_weights_cat)\n",
        "\n",
        "      if mask is not None:\n",
        "          alignment.data.masked_fill_(mask, self.score_mask_value)\n",
        "#에너지 펑션에 소프트맥스를 취해서 어텐션웨이트를 구한다.\n",
        "      attention_weights = F.softmax(alignment, dim=1)\n",
        "      #인코더 임베딩 값들과 매트릭스 멀티플케이션을 하여 콘텍스트 벡터를 얻는다.\n",
        "      attention_context = torch.bmm(attention_weights.unsqueeze(1), memory) # bmm: batch matrix multiplication\n",
        "      attention_context = attention_context.squeeze(1)\n",
        "#출력에서는 컨텍스트 벡터와 얼라이 어텐션 레이트 (수식에서 봤던y값) 두개를 출력으로 내보내게 된다.\n",
        "      return attention_context, attention_weights"
      ],
      "metadata": {
        "id": "NKw3pbqGAFk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3-3) Decoder"
      ],
      "metadata": {
        "id": "5jbg7qjhAIOO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#프리넷 같은 경우에는 각각의 레이어를 쌓는 형태로 되어 있다.\n",
        "class Prenet(nn.Module):\n",
        "  def __init__(self, in_dim, sizes):\n",
        "      super(Prenet, self).__init__()\n",
        "      in_sizes = [in_dim] + sizes[:-1]\n",
        "      self.layers = nn.ModuleList(\n",
        "          [LinearNorm(in_size, out_size, bias=False)\n",
        "            for (in_size, out_size) in zip(in_sizes, sizes)])\n",
        "#각각의 레이어는 드롭아웃을 거치게 된다는 것을 알 수 있다.\n",
        "#엑티베이션으로는 렐루를 사용하여 프리넷을 하게 된다.\n",
        "  def forward(self, x):\n",
        "      for linear in self.layers:\n",
        "          x = F.dropout(F.relu(linear(x)), p=0.5, training=True)\n",
        "      return x\n",
        "\n",
        "##포스트넷 같은경우에는 1d컨볼루션을 쌓는 형태로 되어 있다.\n",
        "class Postnet(nn.Module):\n",
        "  \"\"\"Postnet\n",
        "      - Five 1-d convolution with 512 channels and kernel size 5\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, hparams):\n",
        "      super(Postnet, self).__init__()\n",
        "      self.convolutions = nn.ModuleList()\n",
        "\n",
        "      self.convolutions.append(\n",
        "          nn.Sequential(\n",
        "              ConvNorm(hparams.n_mel_channels, hparams.postnet_embedding_dim,\n",
        "                        kernel_size=hparams.postnet_kernel_size, stride=1,\n",
        "                        padding=int((hparams.postnet_kernel_size - 1) / 2),\n",
        "                        dilation=1, w_init_gain='tanh'),\n",
        "              nn.BatchNorm1d(hparams.postnet_embedding_dim))\n",
        "      )\n",
        "\n",
        "      for i in range(1, hparams.postnet_n_convolutions - 1):\n",
        "          self.convolutions.append(\n",
        "              nn.Sequential(\n",
        "                  ConvNorm(hparams.postnet_embedding_dim,\n",
        "                            hparams.postnet_embedding_dim,\n",
        "                            kernel_size=hparams.postnet_kernel_size, stride=1,\n",
        "                            padding=int((hparams.postnet_kernel_size - 1) / 2),\n",
        "                            dilation=1, w_init_gain='tanh'),\n",
        "                  nn.BatchNorm1d(hparams.postnet_embedding_dim))\n",
        "          )\n",
        "\n",
        "      self.convolutions.append(\n",
        "          nn.Sequential(\n",
        "              ConvNorm(hparams.postnet_embedding_dim, hparams.n_mel_channels,\n",
        "                        kernel_size=hparams.postnet_kernel_size, stride=1,\n",
        "                        padding=int((hparams.postnet_kernel_size - 1) / 2),\n",
        "                        dilation=1, w_init_gain='linear'),\n",
        "              nn.BatchNorm1d(hparams.n_mel_channels))\n",
        "          )\n",
        "#마지막 컨볼루션 레이어 전까지는 하이퍼 탄젠트를 엑티베이션으로 하는 1D컨볼루션을 쌓고 그때마다 드롭아웃을 수행하게 되고\n",
        "#마지막 레이어에서는 그냥 멜을 리그레션 할 수 있는 형태로 리니어가 나오게 된다.\n",
        "#레이어마다 드롭아웃을 전부 했었는데 드롭아웃에 관한건 코드에 따라서 많이 차이가 있는 것 같다.어느 코드는 인코더 부분에만 드롭아웃이 달려있다(논문에도 디테일하게 나와있지 않음,성능에 따라 사용하는 듯)\n",
        "#노멀라이 제이션이나 드롭아웃을 군데군데 사용하면 성능이 좋아지는 것을 확인했다.옵티마이저하는 과정을 거쳐서 이것 저것 바꿔보며 실험을 해봐야 할 듯.\n",
        "  def forward(self, x):\n",
        "      for i in range(len(self.convolutions) - 1):\n",
        "          x = F.dropout(torch.tanh(self.convolutions[i](x)), 0.5, self.training)\n",
        "      x = F.dropout(self.convolutions[-1](x), 0.5, self.training)\n",
        "\n",
        "      return x\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, hparams):\n",
        "      super(Decoder, self).__init__()\n",
        "      #메스펙토그램을 몇차로 할 것인지\n",
        "      self.n_mel_channels = hparams.n_mel_channels\n",
        "      #몇개의 멜을 출력으로 할 것인지(타코트론2에서는 1)\n",
        "      self.n_frames_per_step = hparams.n_frames_per_step\n",
        "      #인코더 임베디드를 입력으로 받기 때문에 그 디멘션\n",
        "      self.encoder_embedding_dim = hparams.encoder_embedding_dim\n",
        "      \n",
        "      self.attention_rnn_dim = hparams.attention_rnn_dim\n",
        "      self.decoder_rnn_dim = hparams.decoder_rnn_dim\n",
        "      self.prenet_dim = hparams.prenet_dim\n",
        "      self.max_decoder_steps = hparams.max_decoder_steps\n",
        "      self.gate_threshold = hparams.gate_threshold\n",
        "      self.p_attention_dropout = hparams.p_attention_dropout\n",
        "      self.p_decoder_dropout = hparams.p_decoder_dropout\n",
        "\n",
        "      self.prenet = Prenet(\n",
        "          hparams.n_mel_channels * hparams.n_frames_per_step,\n",
        "          [hparams.prenet_dim, hparams.prenet_dim])\n",
        "\n",
        "      self.attention_rnn = nn.LSTMCell(\n",
        "          hparams.prenet_dim + hparams.encoder_embedding_dim,\n",
        "          hparams.attention_rnn_dim)\n",
        "\n",
        "      self.attention_layer = Attention(\n",
        "          hparams.attention_rnn_dim, hparams.encoder_embedding_dim,\n",
        "          hparams.attention_dim, hparams.attention_location_n_filters,\n",
        "          hparams.attention_location_kernel_size)\n",
        "\n",
        "      self.decoder_rnn = nn.LSTMCell(\n",
        "          hparams.attention_rnn_dim + hparams.encoder_embedding_dim,\n",
        "          hparams.decoder_rnn_dim, 1)\n",
        "#멜을 추정해야 되기 때문에 엑티베이션이 그냥 리니어이다\n",
        "      self.linear_projection = LinearNorm(\n",
        "          hparams.decoder_rnn_dim + hparams.encoder_embedding_dim,\n",
        "          hparams.n_mel_channels * hparams.n_frames_per_step)\n",
        "#게이트는 스탑토큰을 프레딕션 해야 하기 때문에 시그모이드로 출력을 받게 된다.\n",
        "      self.gate_layer = LinearNorm(\n",
        "          hparams.decoder_rnn_dim + hparams.encoder_embedding_dim, 1,\n",
        "          bias=True, w_init_gain='sigmoid')\n",
        "#첫 번째 디코더로 들어갈 0들을 만드는 작업.\n",
        "  def get_go_frame(self, memory):\n",
        "      \"\"\" Gets all zeros frames to use as first decoder input\n",
        "      PARAMS\n",
        "      ------\n",
        "      memory: decoder outputs\n",
        "\n",
        "      RETURNS\n",
        "      -------\n",
        "      decoder_input: all zeros frames\n",
        "      \"\"\"\n",
        "\n",
        "      B = memory.size(0)\n",
        "      decoder_input = Variable(memory.data.new(\n",
        "          B, self.n_mel_channels * self.n_frames_per_step).zero_())\n",
        "      return decoder_input\n",
        "#어텐션 스테이트 디코더 스테이트들을 이 코드에서 정의해준다.\n",
        "  def initialize_decoder_states(self, memory, mask):\n",
        "      \"\"\" Initializes attention rnn states, decoder rnn states, attention\n",
        "      weights, attention cumulative weights, attention context, stores memory\n",
        "      and stores processed memory\n",
        "      PARAMS\n",
        "      ------\n",
        "      memory: Encoder outputs\n",
        "      mask: Mask for padded data if training, expects None for inference\n",
        "      \"\"\"\n",
        "      B = memory.size(0)\n",
        "      MAX_TIME = memory.size(1)\n",
        "\n",
        "      self.attention_hidden = Variable(memory.data.new(\n",
        "          B, self.attention_rnn_dim).zero_())\n",
        "      self.attention_cell = Variable(memory.data.new(\n",
        "          B, self.attention_rnn_dim).zero_())\n",
        "\n",
        "      self.decoder_hidden = Variable(memory.data.new(\n",
        "          B, self.decoder_rnn_dim).zero_())\n",
        "      self.decoder_cell = Variable(memory.data.new(\n",
        "          B, self.decoder_rnn_dim).zero_())\n",
        "\n",
        "      self.attention_weights = Variable(memory.data.new(\n",
        "          B, MAX_TIME).zero_())\n",
        "      self.attention_weights_cum = Variable(memory.data.new(\n",
        "          B, MAX_TIME).zero_())\n",
        "      self.attention_context = Variable(memory.data.new(\n",
        "          B, self.encoder_embedding_dim).zero_())\n",
        "\n",
        "      self.memory = memory\n",
        "      self.processed_memory = self.attention_layer.memory_layer(memory)\n",
        "      self.mask = mask\n",
        "#피쳐 푸싱에 관련된 내용이다. 타코트론 디코더를 학습할 때에 추정된 매를 다음번에 입력에 넣어주는게 아니라 정답 멜을 사용하는데\n",
        "#그 부분을 담당하는게 파스 디코더 인풋이다.\n",
        "  def parse_decoder_inputs(self, decoder_inputs):\n",
        "      \"\"\" Prepares decoder inputs, i.e. mel outputs\n",
        "      PARAMS\n",
        "      ------\n",
        "      decoder_inputs: inputs used for teacher-forced training, i.e. mel-specs\n",
        "\n",
        "      RETURNS\n",
        "      -------\n",
        "      inputs: processed decoder inputs\n",
        "\n",
        "      \"\"\"\n",
        "      # (B, n_mel_channels, T_out) -> (B, T_out, n_mel_channels)\n",
        "      #추정된 것 대신 대신 넣는 것에 대해 구현되어 있다.\n",
        "      #트랜스포즈는 디멘션을 맞춰주기 위해 진행되는 것이라 보면 된다.\n",
        "      decoder_inputs = decoder_inputs.transpose(1, 2)\n",
        "      decoder_inputs = decoder_inputs.view(\n",
        "          decoder_inputs.size(0),\n",
        "          int(decoder_inputs.size(1)/self.n_frames_per_step), -1)\n",
        "      # (B, T_out, n_mel_channels) -> (T_out, B, n_mel_channels)\n",
        "      decoder_inputs = decoder_inputs.transpose(0, 1)\n",
        "      return decoder_inputs\n",
        "# 이 부분은 디코더 아웃풋을 어떻게 파싱할지 하는 부분.\n",
        "  def parse_decoder_outputs(self, mel_outputs, gate_outputs, alignments):\n",
        "      \"\"\" Prepares decoder outputs for output\n",
        "      PARAMS\n",
        "      ------\n",
        "      mel_outputs:\n",
        "      gate_outputs: gate output energies\n",
        "      alignments:\n",
        "\n",
        "      RETURNS\n",
        "      -------\n",
        "      mel_outputs:\n",
        "      gate_outpust: gate output energies\n",
        "      alignments:\n",
        "      \"\"\"\n",
        "      # (T_out, B) -> (B, T_out)\n",
        "      alignments = torch.stack(alignments).transpose(0, 1)\n",
        "      # (T_out, B) -> (B, T_out)\n",
        "      gate_outputs = torch.stack(gate_outputs).transpose(0, 1)\n",
        "      gate_outputs = gate_outputs.contiguous()\n",
        "      # (T_out, B, n_mel_channels) -> (B, T_out, n_mel_channels)\n",
        "      mel_outputs = torch.stack(mel_outputs).transpose(0, 1).contiguous()\n",
        "      # decouple frames per step\n",
        "      mel_outputs = mel_outputs.view(\n",
        "          mel_outputs.size(0), -1, self.n_mel_channels)\n",
        "      # (B, T_out, n_mel_channels) -> (B, n_mel_channels, T_out)\n",
        "      mel_outputs = mel_outputs.transpose(1, 2)\n",
        "\n",
        "      return mel_outputs, gate_outputs, alignments\n",
        "\n",
        "  def decode(self, decoder_input):\n",
        "      \"\"\" Decoder step using stored states, attention and memory\n",
        "      PARAMS\n",
        "      ------\n",
        "      decoder_input: previous mel output\n",
        "\n",
        "      RETURNS\n",
        "      -------\n",
        "      mel_output:\n",
        "      gate_output: gate output energies\n",
        "      attention_weights:\n",
        "      \"\"\"\n",
        "      #기본적으로 디코더에 인풋으로 들어가는 것은 어텐션에 컨텍스트 벡터를 콘텍트에서 쓰게 된다. 컨텍트 하는 부분.\n",
        "      #cell_input은 어텐션rnn 디코션 rnn 에 들어가는 인풋을 결정해 주는 것이다.\n",
        "      #이 인풋은 전 시간의 디코더 아웃풋이 디코더 인풋으로 들어가고 어텐션 컨텍스트 벡터의 결과를 같이 컨켁트네이션을하여 같이 사용한다.\n",
        "      cell_input = torch.cat((decoder_input, self.attention_context), -1)\n",
        "      #어텐션 rnn을 통해서 한 번 LSTM을 통과하게된다, 어텐션 rnn이 들어가는 것을 볼 수 있다.\n",
        "      self.attention_hidden, self.attention_cell = self.attention_rnn(\n",
        "          cell_input, (self.attention_hidden, self.attention_cell))\n",
        "      #dropout이 들어간다\n",
        "      self.attention_hidden = F.dropout(\n",
        "          self.attention_hidden, self.p_attention_dropout, self.training)\n",
        "\n",
        "      attention_weights_cat = torch.cat(\n",
        "          (self.attention_weights.unsqueeze(1),\n",
        "            self.attention_weights_cum.unsqueeze(1)), dim=1)\n",
        "      #어텐션레이어가 나오게 되는데 어텐션 레이어는 아까 말한대로 어텐션히든, ht가 입력으로 들어가고 메모리는 인코더 인베딩 시퀀스라 보면 된다.\n",
        "      self.attention_context, self.attention_weights = self.attention_layer(\n",
        "          self.attention_hidden, self.memory, self.processed_memory,\n",
        "          attention_weights_cat, self.mask) #전시간의 어텐션을 넣어준다고 보면된다\n",
        "#어텐션웨이트들이 나올때마다 더해줘서 모노토닉한 부분을 억지로 조금이라도 넣어줄 수 있도록 하는 방식이 타코트론2에서 사용된 방식이다.\n",
        "      self.attention_weights_cum += self.attention_weights\n",
        "      decoder_input = torch.cat(\n",
        "      #어텐션히든과 어텐션컨텍스트 두 가지 어텐션LSTM과 어텐션 자체에서 나온 컨텍스트 벡터 이렇게 2가지를 concat에서 사용하게된다.\n",
        "          (self.attention_hidden, self.attention_context), -1)\n",
        "      self.decoder_hidden, self.decoder_cell = self.decoder_rnn(\n",
        "          decoder_input, (self.decoder_hidden, self.decoder_cell))\n",
        "      self.decoder_hidden = F.dropout(\n",
        "          self.decoder_hidden, self.p_decoder_dropout, self.training)\n",
        "#리니어 프로젝션 멜을 뽑는 리니어 프로젝션과 스탑토큰 프레딕션을 할 때 디코더 LSTM값에 디코더히든과 어텐션콘텍스트벡터를 콘캣하게된다\n",
        "      decoder_hidden_attention_context = torch.cat(\n",
        "          (self.decoder_hidden, self.attention_context), dim=1)\n",
        "      decoder_output = self.linear_projection(\n",
        "          decoder_hidden_attention_context)\n",
        "      gate_prediction = self.gate_layer(decoder_hidden_attention_context)\n",
        "      ##최종적으로 디코더 아웃풋 멜이 나오게되고, gate는 스탑토큰,attention은얼라인과 관련된 이렇게 3 가지가 출력으로 나오게된다.\n",
        "      return decoder_output, gate_prediction, self.attention_weights\n",
        "\n",
        "  def forward(self, memory, decoder_inputs, memory_lengths):\n",
        "      \"\"\" Decoder forward pass for training\n",
        "      PARAMS\n",
        "      ------\n",
        "      memory: Encoder outputs\n",
        "      decoder_inputs: Decoder inputs for teacher forcing. i.e. mel-specs\n",
        "      memory_lengths: Encoder output lengths for attention masking.\n",
        "\n",
        "      RETURNS\n",
        "      -------\n",
        "      mel_outputs: mel outputs from the decoder\n",
        "      gate_outputs: gate outputs from the decoder\n",
        "      alignments: sequence of attention weights from the decoder\n",
        "      \"\"\"\n",
        "#학습시 0을 처음값으로 주고 그 인풋을 파싱한 값을 준비한다음 프리넷에 통과시켜 학습하게된다.\n",
        "#실제 그라운드 트루 스멜 스펙트로그램을 인풋으로 넣어주는 과정이라 보면 된다.\n",
        "      decoder_input = self.get_go_frame(memory).unsqueeze(0)\n",
        "      decoder_inputs = self.parse_decoder_inputs(decoder_inputs)\n",
        "      decoder_inputs = torch.cat((decoder_input, decoder_inputs), dim=0)\n",
        "      decoder_inputs = self.prenet(decoder_inputs)\n",
        "\n",
        "      self.initialize_decoder_states(\n",
        "          memory, mask=~get_mask_from_lengths(memory_lengths))\n",
        "\n",
        "      mel_outputs, gate_outputs, alignments = [], [], []\n",
        "      while len(mel_outputs) < decoder_inputs.size(0) - 1:\n",
        "          decoder_input = decoder_inputs[len(mel_outputs)]\n",
        "          #디코드를통해 멜 아웃풋 게이트 아웃풋 어탠션 웨이트를 구하는 과정이라 보면된다.\n",
        "          mel_output, gate_output, attention_weights = self.decode(\n",
        "              decoder_input)\n",
        "          mel_outputs += [mel_output.squeeze(1)]\n",
        "          gate_outputs += [gate_output.squeeze(1)]\n",
        "          alignments += [attention_weights]\n",
        "      #구해진 값을 파싱을 통해 다시 정리를 하는 하는 과정\n",
        "      mel_outputs, gate_outputs, alignments = self.parse_decoder_outputs(\n",
        "          mel_outputs, gate_outputs, alignments)\n",
        "\n",
        "      return mel_outputs, gate_outputs, alignments\n",
        "\n",
        "  def inference(self, memory):\n",
        "      \"\"\" Decoder inference\n",
        "      PARAMS\n",
        "      ------\n",
        "      memory: Encoder outputs\n",
        "\n",
        "      RETURNS\n",
        "      -------\n",
        "      mel_outputs: mel outputs from the decoder\n",
        "      gate_outputs: gate outputs from the decoder\n",
        "      alignments: sequence of attention weights from the decoder\n",
        "      \"\"\"\n",
        "      #티쳐포싱을 더이상 쓸 수 없기 때문에 그냥 프리모드. 즉 전 시간의 디코더 인풋을 말문을 통해 받는 과정\n",
        "      decoder_input = self.get_go_frame(memory)\n",
        "\n",
        "      self.initialize_decoder_states(memory, mask=None)\n",
        "\n",
        "      mel_outputs, gate_outputs, alignments = [], [], []\n",
        "      \n",
        "      while True: #free learning 모드\n",
        "          decoder_input = self.prenet(decoder_input)\n",
        "          mel_output, gate_output, alignment = self.decode(decoder_input)\n",
        "\n",
        "          mel_outputs += [mel_output.squeeze(1)]\n",
        "          gate_outputs += [gate_output]\n",
        "          alignments += [alignment]\n",
        "      #말문이 끝나는 과정은 스탑토큰의 결과가 쓰레쉬홀드를 넘을 경우 브레이크 해버리게 되는 형태로 합성이 이루어진다.\n",
        "          if torch.sigmoid(gate_output.data) > self.gate_threshold:\n",
        "              break\n",
        "          elif len(mel_outputs) == self.max_decoder_steps:\n",
        "              print(\"Warning! Reached max decoder steps\")\n",
        "              break\n",
        "#학습할 때에는 브레이크 없이 쭉 값들은 받고 그 값을 로스펑션할대만 사용한다.\n",
        "          decoder_input = mel_output\n",
        "\n",
        "      mel_outputs, gate_outputs, alignments = self.parse_decoder_outputs(\n",
        "          mel_outputs, gate_outputs, alignments)\n",
        "\n",
        "      return mel_outputs, gate_outputs, alignments\n",
        "#스탑토큰이 생각보다 빨리 끊기는 경우가 많은데,즉 '뭐뭐했습니.(다x)'에서 끝난다\n",
        "#보통 뒤가 짤리는 경우는 앞뒤 사일런스를 타이트 하게 잘라서 타코트론이 학습할 때 마지막 발음을 사일런스로 취급하는 경우가 있다.\n",
        "#데이터 프로세싱 할때 트레이닝을 너무 타이트 하게 하면 스탑토큰이 빨리 끊길 수 있다."
      ],
      "metadata": {
        "id": "U0pCH3RNAJdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3-4) Tacotron2"
      ],
      "metadata": {
        "id": "jnclxuKMAQKS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#전체적인 타코트론2에 대한 구조\n",
        "#각각의 인코더 디코더 포스트넷을 불러온다.\n",
        "class Tacotron2(nn.Module):\n",
        "  def __init__(self, hparams):\n",
        "      super(Tacotron2, self).__init__()\n",
        "      self.mask_padding = hparams.mask_padding\n",
        "      self.n_mel_channels = hparams.n_mel_channels\n",
        "      self.n_frames_per_step = hparams.n_frames_per_step\n",
        "      self.embedding = nn.Embedding(\n",
        "          len(vocab), hparams.symbols_embedding_dim)\n",
        "      std = sqrt(2.0 / (len(vocab) + hparams.symbols_embedding_dim))\n",
        "      val = sqrt(3.0) * std  # uniform bounds for std\n",
        "      self.embedding.weight.data.uniform_(-val, val)\n",
        "      self.encoder = Encoder(hparams)\n",
        "      self.decoder = Decoder(hparams)\n",
        "      self.postnet = Postnet(hparams)\n",
        "#배치를 파싱 어느코드랑 다 비슷비슷하다\n",
        "  def parse_batch(self, batch):\n",
        "      text_padded, input_lengths, mel_padded, gate_padded, \\\n",
        "          output_lengths = batch\n",
        "      text_padded = to_gpu(text_padded).long()\n",
        "      input_lengths = to_gpu(input_lengths).long()\n",
        "      max_len = torch.max(input_lengths.data).item()\n",
        "      mel_padded = to_gpu(mel_padded).float()\n",
        "      gate_padded = to_gpu(gate_padded).float()\n",
        "      output_lengths = to_gpu(output_lengths).long()\n",
        "\n",
        "      return (\n",
        "          (text_padded, input_lengths, mel_padded, max_len, output_lengths),\n",
        "          (mel_padded, gate_padded))\n",
        "#매스크를 사용하여 패딩된걸 날린다.\n",
        "  def parse_output(self, outputs, output_lengths=None):\n",
        "      if self.mask_padding and output_lengths is not None:\n",
        "          mask = ~get_mask_from_lengths(output_lengths)\n",
        "          mask = mask.expand(self.n_mel_channels, mask.size(0), mask.size(1))\n",
        "          mask = mask.permute(1, 0, 2)\n",
        "\n",
        "          outputs[0].data.masked_fill_(mask, 0.0)\n",
        "          outputs[1].data.masked_fill_(mask, 0.0)\n",
        "          outputs[2].data.masked_fill_(mask[:, 0, :], 1e3)  # gate energies\n",
        "\n",
        "      return outputs\n",
        "#텍스트가 로딩을하게된다 mels, max_len등\n",
        "  def forward(self, inputs):\n",
        "      text_inputs, text_lengths, mels, max_len, output_lengths = inputs\n",
        "      text_lengths, output_lengths = text_lengths.data, output_lengths.data\n",
        "#이 임베딩은 학습가능한 루거테이블이라 보면된다.\n",
        "      embedded_inputs = self.embedding(text_inputs).transpose(1, 2) #LUT\n",
        "#인풋아웃풋통과\n",
        "      encoder_outputs = self.encoder(embedded_inputs, text_lengths)\n",
        "\n",
        "      mel_outputs, gate_outputs, alignments = self.decoder(\n",
        "          encoder_outputs, mels, memory_lengths=text_lengths)\n",
        "#여기있는 포스트넷은 레지듀얼커넥션을 가지게 되서 이렇게 표현한다.\n",
        "      mel_outputs_postnet = self.postnet(mel_outputs)\n",
        "      mel_outputs_postnet = mel_outputs + mel_outputs_postnet\n",
        "#파스아웃풋을통해 매스함수를 씌워주게된다\n",
        "      return self.parse_output(\n",
        "          [mel_outputs, mel_outputs_postnet, gate_outputs, alignments],\n",
        "          output_lengths)\n",
        "\n",
        "  def inference(self, inputs):\n",
        "      embedded_inputs = self.embedding(inputs).transpose(1, 2)\n",
        "      encoder_outputs = self.encoder.inference(embedded_inputs)\n",
        "      mel_outputs, gate_outputs, alignments = self.decoder.inference(\n",
        "          encoder_outputs)\n",
        "\n",
        "      mel_outputs_postnet = self.postnet(mel_outputs)\n",
        "      mel_outputs_postnet = mel_outputs + mel_ouftputs_postnet\n",
        "\n",
        "      outputs = self.parse_output(\n",
        "          [mel_outputs, mel_outputs_postnet, gate_outputs, alignments])\n",
        "\n",
        "      return outputs"
      ],
      "metadata": {
        "id": "4nc_ympdASJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. 모델 선언 및 옵티마이저 설정"
      ],
      "metadata": {
        "id": "gWdc7k6bAUdI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Tacotron2(hparams).to(device)\n",
        "#위에서 설정했던 파라미터 값들.\n",
        "learning_rate = hparams.learning_rate\n",
        "#타코트론 같은 경우는 아담 옵티마이저를 사용한다라고 볼 수 있다.\n",
        "#성능 자체는 아담이 제일 안정적으로 나온다, 구현상이나 코드에 따라서 어느정도 차이가 있을 수 있다.\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,\n",
        "                              weight_decay=hparams.weight_decay)"
      ],
      "metadata": {
        "id": "n97mVre5AWV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.목적함수정의"
      ],
      "metadata": {
        "id": "7Vbxh2N08IDc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Tacotron2Loss(nn.Module):\n",
        "  def __init__(self):\n",
        "      super(Tacotron2Loss, self).__init__()\n",
        "#타겟자체는 두개가 있다, 멜타겟= 최종적인 출력, 게이트타겟=스탑토큰타겟, \n",
        "#둘 다 그래디언트가 필요없기 때문에 False로 정의해준다.\n",
        "  def forward(self, model_output, targets):\n",
        "      mel_target, gate_target = targets[0], targets[1]\n",
        "      mel_target.requires_grad = False\n",
        "      gate_target.requires_grad = False\n",
        "      gate_target = gate_target.view(-1, 1)\n",
        "#모델 아웃풋은 포스넷을 걸치지 않은 멜 아웃풋, 포스넷을 걸친 최종 아웃풋, 게이트 아웃풋 3가지를 사용하게된다.\n",
        "      mel_out, mel_out_postnet, gate_out, _ = model_output\n",
        "      gate_out = gate_out.view(-1, 1)\n",
        "      #멜 로스 같은 경우에는 비포어 로스, 에프터 로스 이것을 MSE를 통해 더해준다\n",
        "      mel_loss = nn.MSELoss()(mel_out, mel_target) + nn.MSELoss()(mel_out_postnet, mel_target)\n",
        "      #게이트 로스도 바이너리 크로센트리를 사용하게 된다, 바이너리 크로센트리 엔트리는 BCE보다 안정적이여서 많이 쓴다\n",
        "      gate_loss = nn.BCEWithLogitsLoss()(gate_out, gate_target) #binary cross entropy (BCE보다 안정적)\n",
        "      #총 3가지 로스가 합쳐진 형태.\n",
        "      return mel_loss + gate_loss\n",
        "\n",
        "criterion = Tacotron2Loss()"
      ],
      "metadata": {
        "id": "yjI7fj3X8Jnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. 훈련"
      ],
      "metadata": {
        "id": "fPVUlCrQ8KRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iteration = 0\n",
        "#보통 데이터 사이즈에 따라서 에폭이 다르다, 100k정도만 돌아도 소리가 잘 난다,블리자드 같은경우 150정도 ,모델 사이즈나 데이터 사이즈에 따라서 유동적으로 잡자.\n",
        "#중간중간에 샘플을 들어보면 굉장히 잘 나온다 정답 멜들이 계속 들어오기 때문에 하지만 테스트했을 때 잘 나올거라 생각하면 안된다.\n",
        "#보통의경우는 얼라이먼트커브를 그린다.여기서는 없다.\n",
        "#어텐션웨이트들을 플랏을 하게 되면 어텐션이 제대로 잡혔는지 안 잡혔는지 알 수 있다.\n",
        "#엔드투엔드는 모노토닉하게 증가해야 된다고 말했는데 그것과 같이 잘 잡힌 것을 알 수 있고 만약 잡히지 않으면 어텐션 웨이트들이 어느 한 인코더값에 집중하지 못하고 날라가는 모습을 보이기도한다.\n",
        "#합성에서는 러닝레이트가 줄어든다고해서 잘 합성이 되고있다고 보면안되고 중간중간 얼라이먼트와 여력이 된다면 실제 합성음을 중간중간 테스트 해보자\n",
        "#합성처음하는 사람들이 러닝레이트가 잘 줄고있어서 학습이 잘 되고 있다고 착각하는 경우가 많다.\n",
        "\n",
        "for epoch in range(100):\n",
        "  print(\"Epoch: {}\".format(epoch))\n",
        "  for i, batch in enumerate(train_loader):\n",
        "    start = time.perf_counter()\n",
        "    model.zero_grad()\n",
        "    x, y = model.parse_batch(batch)\n",
        "    y_pred = model(x)\n",
        "    loss = criterion(y_pred, y)\n",
        "    #로스와 로스를 통한 백프로퍼기게이션을 통해 학습이 되는 단계가 이 부분이다\n",
        "    reduced_loss = loss.item()  #[1.0] -> 1.0\n",
        "    loss.backward()\n",
        "    grad_norm = torch.nn.utils.clip_grad_norm_(\n",
        "        model.parameters(), hparams.grad_clip_thresh)\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    duration = time.perf_counter() - start\n",
        "    print(\"Train loss {} {:.6f} Grad Norm {:.6f} {:.2f}s/it\".format(\n",
        "        iteration, reduced_loss, grad_norm, duration))\n",
        "    \n",
        "    iteration += 1\n",
        "    #타코트론 로스와 같은경우 초반엔 굉장히 빠르게 줄다 빠르게 줄지않는 구간이 금방 찾아온다.\n",
        "    #보코더가 앤드투앤드에서 굉장히 중요한 부분을 차지한다.\n",
        "    #그리핀림과 웨이브넷의 차이가 심한 것 처럼 뉴럴 보코더를 사용하기만해도 성능향상이 엄청나게 이루어진다\n",
        "    #웨이브넷 같은경우에는 연산량문제가 커서 10분짜리 음성을 만드는데 몇십분이 걸리는 경우가 있다 굉장히 느리다.\n",
        "    #현재에 굉장히 성능좋은 보코더가 많이나와서 찾아서 돌려보는 것을 추천한다.\n",
        "    #앤드투앤드라는 의미는 텍스트부터 멜까지가 앤드투 앤드라고 보면되고 일반적으로는 조인 트레이닝 하지않고 웨이브넷 따로 트레이닝 한다.\n",
        "    #웨이브넷 트레이닝 할 때에는 DB 가 달라도 어느 정도 성능이 나오지만 같은 DB를 추천한다.\n",
        "    #멀티 스피커로 넘어가게되면 뉴럴 보코더가 문제가 될 수 있다. 여러 목소리로 합성을 시도해야 하기 떄문에.\n",
        "    #그래서 컨디션해서 넣어주는 방식을 사용하거나 뉴럴보코더를 사용하지않고 그리핀림을 사용하는 방식으로 사용하면 될 것같다\n",
        "    #그리핀림은 음질이 떨어지지만 복원하는 데에는 큰 문제가 없기 때문에 사용하는 것을 추천하고\n",
        "    #성능을 올리고 싶다 하는 사람들은 웨이브넷이나 다른 보코더를 찾아 쓰는게 연구에 도움이 될 듯 하다.\n",
        "    #NVIDIA 깃허브에 있는 코드를 참고하자."
      ],
      "metadata": {
        "id": "AviDUjG-8Mgj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}